---
title: "Raster Speed"
author: "Michael Schmidt"
date: "3/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(raster)
library(sf)
library(dplyr)
```

## Load the imagery path
```{r}
# imagery_path<-"DATA/SanMiguel_West/Tiffs/SM_GypsumGapNW_0103_180611.tif"

imagery_path<-"DATA/Tiffs_W_SM/SM_GypsumGapNW_0103_180611.tif"
r<-brick(imagery_path)
```

## Pre calculation

```{r}
start<-Sys.time()
calcNDVI<-(r[[4]]-r[[1]])/(r[[4]]+r[[1]])
Sys.time()-start
```
The calculation takes `Sys.time()-start`. 
```{r}
rasterOptions(maxmemory = 1e+10)
rasterOptions()
```

```{r}
start<-Sys.time()
calcNDVI<-(r[[4]]-r[[1]])/(r[[4]]+r[[1]])
Sys.time()-start
```
## Increasing chunk size
```{r}
rasterOptions(chunksize = 1e+09)
rasterOptions()
start<-Sys.time()
calcNDVI<-(r[[4]]-r[[1]])/(r[[4]]+r[[1]])
Sys.time()-start
```
This didn't really help probably because you don't need to read into memory for this calculation. 

## Let's try both. 

```{r}
rasterOptions(
  maxmemory = 1e+10,
  chunksize = 1e+09
  )

rasterOptions()

start<-Sys.time()
calcNDVI<-(r[[4]]-r[[1]])/(r[[4]]+r[[1]])
Sys.time()-start

```

## Lets try a function from Raster

First let's reset the raster options.
```{r}
rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

rasterOptions()
```

## Let's try the Scale function
On a subset of the original raster so it doesn't take too long. 
```{r}

start<-Sys.time()
scale_r<-r[[1]]%>%
  raster::scale()
Sys.time()-start
```

## Test increasing maxmemory
```{r}
rm(scale_r)
gc()
rasterOptions()
rasterOptions(
  maxmemory = 9e+10,
  chunksize = 1e+08
)

start<-Sys.time()
scale_r<-r[[1]]%>%
  raster::scale()
Sys.time()-start
```
Wow big performance improvements. One fifth the time on my machine. 

## Now let's test memory

```{r}
rm(scale_r)
gc()
rasterOptions(
  maxmemory = 9e+10,
  chunksize = 1e+09
)

start<-Sys.time()
scale_r<-r[[1]]%>%
  raster::scale()
Sys.time()-start
```
Just a little bit faster than just increasing the memory. 

##Now let's try it with clusterR

To do this lets use two layers from the raster brick.  We'll reset the options and run with normal memory and chunk size as a baseline.   
```{r}
rm(scale_r)
gc()
rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

start<-Sys.time()
scale_r<-r[[1:2]]%>%
  raster::scale()
Sys.time()-start
```
Took about three and a half minutes on my machine. 

## Increased memory without clustering
```{r}
rm(scale_r)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)

start<-Sys.time()

scale_r<-r[[1:2]]%>%
  raster::scale()

Sys.time()-start
```
Way faster. Forty two seconds compared to over three minutes.  

## Now let's try it with clusterR
We'll reset memory. 
```{r}
rm(scale_r)
gc()

rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

start<-Sys.time()
beginCluster(2)
scale_r<-r[[1:2]]%>%
  raster::clusterR(scale)
endCluster()
Sys.time()-start
```
A bit faster than without increasing memory. For whatever reason when I first run this it doesn't work. I have to run it twice. 

## Increase cores and memory
```{r}
rm(scale_r)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)

start<-Sys.time()
beginCluster(2)
scale_r<-raster::clusterR(r[[1:2]], scale)
endCluster()
Sys.time()-start
```
Not that much faster.

# What about if we increase the number of cores.
```{r}
rm(scale_r)
gc()

rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

start<-Sys.time()
beginCluster(4)
scale_r<-r[[1:2]]%>%
  raster::clusterR(scale)
endCluster()
Sys.time()-start
```
Way slower than just increasing memory. 

## Increase cores and memory. 
```{r}
rm(scale_r)
gc()

rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)

start<-Sys.time()
beginCluster(4)
scale_r<-r[[1:2]]%>%
  raster::clusterR(scale)
endCluster()
Sys.time()-start
```
Again, indreasing memory alone is still faster.  But this isn't that big of a raster.  Let's try to do something a bit more memory intensive. 


## Testing the `predict` function. 

I'm going to use predict function to predict a dataset. 
```{r}
clip<-st_read("shapefiles/clip_shape_gypsum_gap_nw_0103.shp")

#height_path<-"DATA/NOC_heights/San_Miguel_West_Raster/SM_GYPSUMGAPNW_0103_180611_dz.tif"
height_path<-"DATA/NOC_heights/WestSM_dZgridded/SM_GYPSUMGAPNW_0103_180611_dz.tif"
h<-raster(height_path)


h_c<-crop(h, clip)%>%
  raster::disaggregate(fact=5)

s<-crop(r, extent(h_c))%>%
  stack(h_c)

names(s)<-c("band1", "band2", "band3", "band4", "height")

model<-readRDS("model_runs/ID_CV_model_BS_UPDATE/ID_CV_model_BS_UPDATE3.rds")

rm(clip, h, h_c, scale_r)
```


## With standard memory
```{r}
gc()
rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

start<-Sys.time()

predict<-raster::predict(s, model)

Sys.time()-start

```
About seven and a half minutes on my machine. 

## Now with increased memory
```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)

start<-Sys.time()

predict<-raster::predict(s, model)

Sys.time()-start

```
Interesting.  Almost no difference. They both take about 7 minutes. 

## Chunksize increase
```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+09
)

start<-Sys.time()

predict<-raster::predict(s, model)

Sys.time()-start

```


## Multi Core
```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+09,
  chunksize = 1e+08
)

start<-Sys.time()

beginCluster(4)
predict<-raster::clusterR(s, predict, args = list(model = model))
endCluster()
Sys.time()-start

```
Twice as fast.  Now let's try to increase the memory and the cores. 

## Multi Core with added memory
```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 1e+11,
  chunksize = 1e+08
)

start<-Sys.time()

beginCluster(4)
predict<-raster::clusterR(s, predict, args = list(model = model))
endCluster()
Sys.time()-start

```
No help from the added memory. 

?What about chunk size?
```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+09
)

start<-Sys.time()

beginCluster(4)
predict<-raster::clusterR(s, predict, args = list(model = model))
endCluster()
Sys.time()-start

```
Slower.  Just for kicks lets try decreasing chunk size. 

```{r}
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+06
)

start<-Sys.time()

beginCluster(4)
predict<-raster::clusterR(s, predict, args = list(model = model))
endCluster()
Sys.time()-start
```
No help. 

## Summary
It seems like for `scale()` increasing memory really helps, but with `predict()` using multiple cores really helps.   

## Experimenting with {doParallel}

the last thing I want to experiment with is doParallel. 
```{r}
library(doParallel)
library(tidyverse)
library(raster)
```
## Let's try to 

I'll reread in the large raster that I removed above. 
```{r}
imagery_path<-"DATA/SanMiguel_West/Tiffs/SM_GypsumGapNW_0103_180611.tif"
r<-brick(imagery_path)
```

Now lets build a regular for `for` loop and run it 4 times. This is going to take a long time. 

```{r}
n<-4
rm(predict)
gc()
rasterOptions(
  maxmemory = 5e+9,
  chunksize = 1e+08
)
start<-Sys.time()
final_list<-vector(mode = "list", length = n)
for(i in seq(1:n)){
  print(i)
  print(Sys.time())
  raster::scale(r[[1:2]])
  
  final_list[[i]]<-raster::predict(s, model)
}
Sys.time()-start
```
As I suspected this took a long time, 43 minutes. That's a long time. 

Let's see how much faster a `foreach` loop is using 4 cores with the same amount of memory.  
```{r}
n<-4

rm(final_list)
gc()

rasterOptions(
  maxmemory = 5e+9,
  chunksize = 1e+08
)

final_list<-vector(mode = "list", length = n)

start<-Sys.time()

cl<-makePSOCKcluster(4) ##if you're on a mac, this should be makeCluster()

registerDoParallel(cl)

returned_list<-foreach(i = 1:length(final_list)) %dopar% {
  print(i)
  print(Sys.time())
  raster::scale(r[[1:2]])
  
  final_list[[i]]<-raster::predict(s, model)
}
stopCluster(cl)

Sys.time()-start
```
Way faster.  Took about 14 minutes. One thing, the print statements do nothing. And at first I didn't add the `returned_list` part so id did nothing. So keep in mind this is slightly diferent than your typical for loop. 

Now let's try to increase memory and use doParallel. On the last go round My machine was not allowing the machine to go over the memory limit. 

```{r}
n<-4

rm(final_list)
gc()

rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)

final_list<-vector(mode = "list", length = n)

start<-Sys.time()

cl<-makePSOCKcluster(4) ##if you're on a mac, this should be makeCluster()

registerDoParallel(cl)

returned_list<-foreach(i = 1:length(final_list)) %dopar% {
  print(i)
  print(Sys.time())
  raster::scale(r[[1:2]])
  
  final_list[[i]]<-raster::predict(s, model)
}
stopCluster(cl)

Sys.time()-start
```
This didn't improve the speed at all.  

It seems like an approach where you wrote a custom for loop would be better that utalizes both `clusterR` and increasing memory. 


```{r}
n<-4
rm(final_list, returned_list)
gc()
rasterOptions(
  maxmemory = 5e+10,
  chunksize = 1e+08
)
start<-Sys.time()
final_list<-vector(mode = "list", length = n)
for(i in seq(1:n)){
  print(i)
  print(Sys.time())
  raster::scale(r[[1:2]])
  
  beginCluster(4)
  final_list[[i]]<-raster::clusterR(s, predict, args = list(model = model))
  endCluster()
}
Sys.time()-start
```

This didn't work quite as well as I expected. `doParallel` sems to be the way to go, here. 












